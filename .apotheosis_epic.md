### Overview

This epic outlines the next major architectural evolution for the Liquid-Hive project. The current v1 system represents a best-in-class reactive autonomous agent, capable of learning from user interactions. The goal of Project Apotheosis is to transform Liquid-Hive into a proactive cognitive architecture—a system that can reason about its own knowledge, identify its own weaknesses, and strategically direct its own growth.

This initiative is divided into three core pillars:
1. Cognitive Core Enhancements: Upgrading the fundamental reasoning and memory capabilities.
2. Autonomy & Self-Improvement Enhancements: Accelerating the learning loop and breaking the system’s dependency on its Oracles.
3. Supporting MLOps & HCI Enhancements: Building the infrastructure to support and observe this new level of intelligence.

### 1. Cognitive Core Enhancements

- [ ] Implement a Dynamic Reasoning Graph (DAG) Generator
  - Description: Evolve the current StrategySelector into a Planner agent. Instead of selecting a predefined strategy (committee, debate), the Planner will dynamically construct a multi-step execution graph tailored to each prompt. This enables complex, compositional reasoning.
  - Implementation Notes:
    1. Create a new Planner agent within hivemind/.
    2. Design a JSON schema for the reasoning DAG. Nodes should represent actions like tool_call, model_call (specifying a role and prompt), or join (to aggregate results).
    3. Implement an ExecutionEngine that can traverse and execute the DAG, managing the flow of outputs to inputs between steps.
    4. The output of this engine will replace the current hard-coded logic that follows the StrategySelector.

- [ ] Introduce Active Learning via Uncertainty Sampling
  - Description: Make the self-improvement loop proactive. The system should identify its own knowledge gaps and generate “curiosity questions” to fill them, rather than only learning from user interactions.
  - Implementation Notes:
    1. Enhance the AutonomyOrchestrator with a new periodic task: analyze_cognitive_map.
    2. This task reads the cognitive_map.json (or queries Neo4j) to find topics with the lowest confidence scores.
    3. Create a new “Inquisitor” agent role. Its job is to take a low-confidence topic (e.g., “quantum mechanics”) and generate a high-level research question (e.g., “Explain the core principles of quantum entanglement and its potential applications in cryptography.”).
    4. The generated question is then submitted to the existing “Approval Queue” to be actioned by the CuriosityEngine.

- [ ] Implement Memory Compression and Reflection
  - Description: Create a mechanism for the AI to synthesize long-term memories and insights from its short-term episodic logs, similar to human reflection.
  - Implementation Notes:
    1. Create a new background service or a task within the AutonomyOrchestrator called the “Memory Weaver.”
    2. This service reads the raw JSON logs from /data/runs.
    3. It periodically uses a powerful model (e.g., via the Arbiter client) with a reflective prompt, such as: “Based on these interactions, what are the key principles learned? What were my most significant errors and how can I correct them?”
    4. The synthesized insights are written to a high-priority document (e.g., long_term_memory.md) within the RAG ingestion directory (/data/ingest) to be automatically indexed and made available for future reasoning.

### 2. Autonomy & Self-Improvement Enhancements

- [ ] Decouple the Oracle into an “Oracle-as-a-Service” with a Meta-Correction Loop
  - Description: To break the “Oracle Ceiling,” the system must be able to correct its own teacher. This involves externalizing the Oracle/Arbiter pipeline and creating a feedback mechanism.
  - Implementation Notes:
    1. Refactor the Arbiter logic from hivemind/training/arbiter.py into its own standalone FastAPI microservice.
    2. Add a /feedback endpoint to this new service.
    3. In the main Judge agent, if a newly synthesized answer is rated as significantly superior to a previously refined “platinum” answer from the Oracle on the same topic, it triggers a meta_correction event.
    4. This event is posted to the Oracle service’s /feedback endpoint and logged to a special dataset (meta_corrections.jsonl). This dataset becomes the source for fine-tuning a future, more advanced Oracle model.

- [ ] Build an Automated Evaluation Arena
  - Description: Address the evaluation bottleneck by creating an automated benchmark for all new “challenger” adapters. This provides a fast, objective signal of model improvement.
  - Implementation Notes:
    1. Curate a golden_dataset.jsonl containing a set of challenging, representative prompts with ideal reference answers.
    2. Create an “Evaluation Arena” service. When a new challenger adapter is trained, the AutonomyOrchestrator calls this service.
    3. The Arena runs inference with both the current champion and the new challenger across the entire golden dataset.
    4. It uses the Judge agent to perform head-to-head comparisons of their answers and calculates a win rate, Elo score, or similar metric.
    5. This objective score is attached to the promotion proposal in the Approval Queue, giving the operator quantitative data for their decision.

### 3. Supporting MLOps & HCI Enhancements

- [ ] Implement a Graduated Fine-Tuning Pipeline
  - Description: The current SFT process treats all data equally. A graduated pipeline would use different models and techniques for different stages of learning, from foundational knowledge to expert refinement.
  - Implementation Notes:
    1. Modify the hivemind/training/ scripts to support multiple training profiles (e.g., bootstrap, sft, dpo, distillation).
    2. The AutonomyOrchestrator should select a training profile based on the available data. For example:
       - If a large amount of preference data (DPO) is available from Judge rankings, it should trigger a dpo_text.py run.
       - If meta-correction data is available, it could trigger distillation to create a more specialized student model.

- [ ] Enhance the Cerebral GUI for v2 Capabilities
  - Description: The frontend needs to be updated to visualize and interact with the new architectural components.
  - Implementation Notes:
    1. System Panel: Add a component to visualize the Cognitive Map, showing the AI’s confidence across different skills.
    2. Forge Panel: The AdapterDeploymentManager view should be enhanced to show the objective evaluation scores from the Automated Arena for each challenger.
    3. Chat Panel: Add a “Show Reasoning Plan” toggle or button that displays the generated DAG from the new Planner agent for a given query, providing transparency into the AI’s thought process.

### Acceptance Criteria

- [ ] The StrategySelector has been successfully replaced by a Planner agent capable of generating and executing dynamic, multi-step reasoning graphs.
- [ ] The system can autonomously identify a low-confidence topic from its Cognitive Map and place a corresponding research question into the Approval Queue.
- [ ] The system can automatically reflect on past interactions to generate and index long-term memories.
- [ ] The Oracle/Arbiter pipeline is a separate microservice, and the system can log a meta_correction event.
- [ ] New challenger adapters are automatically benchmarked against a golden dataset, with the results displayed in the Cerebral GUI.

### References

- docs/GRADUATION_REPORT.md
- docs/TRAINING_PLAYBOOK.md
- docs/ADDITIONAL_CONSIDERATIONS.md
